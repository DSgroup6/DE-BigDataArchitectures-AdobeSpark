{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6aaffd8c-c5d1-4010-92bb-7c16833f1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col, avg, concat, lit\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, IntegerType, DateType\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import from_json, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "01b4ff54-e517-49a5-a966-431f411859ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataSchema = StructType([\n",
    "#     StructField(\"Case Number\", StringType(), True),\n",
    "#     StructField(\"Date\", DateType(), True),\n",
    "#     StructField(\"Block\", StringType(), True),\n",
    "#     StructField(\"Primary Type\", StringType(), True),\n",
    "#     StructField(\"Description\", StringType(), True),\n",
    "#     StructField(\"District\", StringType(), True),\n",
    "#     StructField(\"Community Area\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "dataSchema = StructType([\n",
    "    # StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Case Number\", StringType(), True),\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Block\", StringType(), True),\n",
    "    StructField(\"Primary Type\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"District\", StringType(), True),\n",
    "    StructField(\"Community Area\", StringType(), True),\n",
    "    StructField(\"weeknumber\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Lab7_1\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "bucket = \"chicagocrime-bigquery-temp-storage\"\n",
    "spark.conf.set(\"temporaryGcsBucket\",bucket)\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a4f69b6c-374b-4403-90fa-3a12fdf302b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format('kafka')\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9093\")\\\n",
    "    .option(\"subscribe\", \"crimes2\") \\\n",
    "    .option('failOnDataLoss', 'false')\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .load()\n",
    "\n",
    "# df = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1) \\\n",
    "#         .csv(\"/home/jovyan/data/crimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "208a2a3a-060c-4b23-826e-d6d2c8b14271",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df.selectExpr('key', 'timestamp','topic','partition','offset','CAST(value AS STRING)')\\\n",
    "    .withColumn('value', from_json(col('value'), dataSchema))\n",
    "\n",
    "    \n",
    "# df_withEventTime = df2.withWatermark(\"timestamp\", \"1 minutes\")\\\n",
    "#     .groupBy(window(col('timestamp'), '10 seconds'), 'value.`Community Area`').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e2b4679e-abe8-4ac7-8179-4bacd9e0b160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weeknumber: integer (nullable = true)\n",
      " |-- Community Area: string (nullable = true)\n",
      " |-- word: string (nullable = false)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split, concat, col, lit\n",
    "words = sdf.select('value.weeknumber','value.year','value.Community Area', explode(split(sdf.value.Description, \" \")).alias(\"word\"))\n",
    "\n",
    "word_count = words.groupBy('year','weeknumber','Community Area', 'word').count()\n",
    "word_count.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "90fc942c-9f0d-4137-a6bb-01b4a6fd9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"group6_chicagocrime\"  #  bucket for the assignment\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "\n",
    "def my_foreach_batch_function(df, batch_id):\n",
    "    df.write.format('bigquery') \\\n",
    "      .option('table', 'datatengineering-group6.crimedescription.wordcounts') \\\n",
    "      .option(\"temporaryGcsBucket\",bucket)\\\n",
    "      .mode(\"append\") \\\n",
    "      .save()\n",
    "    \n",
    "# Write to a sink - here, the output is written to a Big Query Table\n",
    "# Use your gcp bucket name. \n",
    "# ProcessingTime trigger with two-seconds micro-batch interval\n",
    "activityQuery = word_count.writeStream.outputMode(\"update\")\\\n",
    "                    .option(\"checkpointLocation\", \"/home/jovyan/checkpoint/crimes\")\\\n",
    "                    .foreachBatch(my_foreach_batch_function).start() #.trigger(processingTime = '2 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "245e4e36-022b-4482-acdc-9e01e0602866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = word_count.writeStream.queryName(\"count_per_interval\") \\\n",
    "#     .format(\"memory\").outputMode(\"update\") \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b491ddf0-16c2-427b-aa2d-46b10aa6c583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for x in range(100):\n",
    "#     print(q.status)\n",
    "#     spark.sql(\"SELECT * FROM count_per_interval\").show()\n",
    "#     sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4cad9445-0b9f-4be5-9df5-a356650b9d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activityQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6c6a105a-3293-4f05-a547-1f42c52177e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb01a9a-5c5e-4627-88ff-111689ef5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
