{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col, avg, concat, lit\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, IntegerType, DateType\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "\n",
    "# goal of query: get the most commonly used word to describe a crime in each community of chicago\n",
    "#the data is devided into batched of each week of the year 2015, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema = StructType([\n",
    "    # StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Case Number\", StringType(), True),\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Block\", StringType(), True),\n",
    "    StructField(\"Primary Type\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"District\", StringType(), True),\n",
    "    StructField(\"Community Area\", StringType(), True),\n",
    "    StructField(\"weeknumber\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Lab7_1\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# We need to set the following configuration whenever we need to use GCS.\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"group6_chicagocrime\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'gs://chicagocrime_tempstreamingresult'\n",
    "sdf = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1) \\\n",
    "        .csv(path) #\"/home/jovyan/data/crimes2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, concat, col, lit\n",
    "words = sdf.select('weeknumber','year','Community Area', explode(split(sdf.Description, \" \")).alias(\"word\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "\n",
    "#window(col('event_time'), '7 days') Window not needed anymore since batches are split in weeks\n",
    "word_count = words.groupBy('year','weeknumber','Community Area', 'word').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weeknumber: integer (nullable = true)\n",
      " |-- Community Area: string (nullable = true)\n",
      " |-- word: string (nullable = false)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_count.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_foreach_batch_function(df, batch_id):\n",
    "    w = Window.partitionBy('Community Area')\n",
    "    word_count_max = df.withColumn('maxCount', f.max('count').over(w))\\\n",
    "                                           .where(f.col('count') == f.col('maxCount'))\\\n",
    "                                           .drop('maxCount')\n",
    "     \n",
    "    word_count_max.write.format('bigquery') \\\n",
    "      .option('table', 'datatengineering-group6.crimedescription.mostusedwordspercommunity') \\\n",
    "      .mode(\"append\") \\\n",
    "      .save()\n",
    "    \n",
    "# Write to a sink - here, the output is written to a Big Query Table\n",
    "# Use your gcp bucket name. \n",
    "# ProcessingTime trigger with two-seconds micro-batch interval\n",
    "activityQuery = word_count.writeStream.outputMode(\"update\")\\\n",
    "                    .foreachBatch(my_foreach_batch_function).start() #.trigger(processingTime = '2 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "last_mess = None\n",
    "for i in range(3200):\n",
    "    # clear_output(wait=True)\n",
    "    \n",
    "    if activityQuery.status['message'] != last_mess:\n",
    "        print(f'step {i}')\n",
    "        print(activityQuery.status)\n",
    "        last_mess = activityQuery.status['message'] \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0ca3967508393c830e5fdf827e244c60a1e86d5fcaf2e086715c4929113c1ab3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
