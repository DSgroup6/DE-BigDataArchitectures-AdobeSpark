{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col, avg, concat, lit\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, IntegerType\n",
    "from time import sleep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df():\n",
    "\tfrom google.cloud import storage\n",
    "\n",
    "\tstorage_client = storage.Client()\n",
    "\tbucket_name = \"group6_chicagocrime\"\n",
    "\tfile_name = 'sales.csv'\n",
    "\tpd.read_csv(f'gs://{bucket_name}/{file_name}')\n",
    "\t# Creates the new bucket\n",
    "\tbucket = storage_client.create_bucket(bucket_name)\n",
    "\tblob = bucket.blob()\n",
    "\tprint(f\"Bucket {bucket.name} created.\")\n",
    "\n",
    "\twith blob.open(\"r\") as f:\n",
    "\t\tfile = pd.read_csv(f.read())\n",
    "\treturn file\n",
    "\n",
    "\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Lab8_Ex1\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "dataSchema = StructType(\n",
    "    [StructField(\"uname\", StringType(), True),\n",
    "     StructField(\"tname\", StringType(), True),\n",
    "     StructField(\"score\", IntegerType(), True),\n",
    "     StructField(\"timestamp_in_ms\", LongType(), True),\n",
    "     StructField(\"readable_time\", StringType(), True)\n",
    "     ])\n",
    "\n",
    "# Read from a source \n",
    "sdf = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .csv(\"/home/jovyan/data/gamescore\")\n",
    "    \n",
    "    \n",
    "\n",
    "# create the event time column \n",
    "withEventTimedf = sdf.selectExpr(\n",
    "    \"*\",\n",
    "    \"cast(timestamp_in_ms/1000.0 as timestamp) as event_time\")\n",
    "\n",
    "withEventTimedf.printSchema()\n",
    "\n",
    "avgscoredf = withEventTimedf \\\n",
    "    .groupBy(window(col(\"event_time\"), \"10 seconds\"), \"uname\", \"tname\") \\\n",
    "    .agg(avg(\"score\").alias(\"value\"))\n",
    "\n",
    "resultdf = avgscoredf.select(concat(col(\"uname\"), lit(\" \"), col(\"tname\")).alias(\"key\"), col(\"value\"))\n",
    "\n",
    "query = resultdf \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"avg_score_window\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "\n",
    "try:\n",
    "    for x in range(100):\n",
    "        spark.sql(\"SELECT * FROM avg_score_window\").show()\n",
    "        sleep(10)\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
