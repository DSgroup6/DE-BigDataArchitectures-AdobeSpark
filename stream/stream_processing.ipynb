{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col, avg, concat, lit\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, IntegerType\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from google.cloud import pubsub_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"CrimeStreamProcessing\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "project_number = 868393134342\n",
    "location = \"us-central1-a\"\n",
    "subscription_id = \"crimes-subscription\"\n",
    "\n",
    "sdf = (\n",
    "    spark.readStream.format(\"pubsublite\")\n",
    "    .option(\n",
    "        \"pubsublite.subscription\",\n",
    "        f\"projects/{project_number}/locations/{location}/subscriptions/{subscription_id}\",\n",
    "    )\n",
    "    .load()\n",
    ")\n",
    "\n",
    "sdf = sdf.withColumn(\"data\", sdf.data.cast(StringType()))\n",
    "\n",
    "print(sdf.head())\n",
    "\n",
    "query = (\n",
    "    sdf.writeStream.format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination(60)\n",
    "query.stop()\n",
    "\n",
    "\n",
    "# ssc = StreamingContext(sparkConf, 28) # https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with pubsub_v1.SubscriberClient() as subscriber:\n",
    "#     subscriber.create_subscription(\n",
    "#         name=subscription_name, topic=topic_name)\n",
    "#     future = subscriber.subscribe(subscription_name, callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataSchema = StructType(\n",
    "#     [StructField(\"uname\", StringType(), True),\n",
    "#      StructField(\"tname\", StringType(), True),\n",
    "#      StructField(\"score\", IntegerType(), True),\n",
    "#      StructField(\"timestamp_in_ms\", LongType(), True),\n",
    "#      StructField(\"readable_time\", StringType(), True)\n",
    "#      ])\n",
    "\n",
    "# # Read from a source \n",
    "# sdf = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1) \\\n",
    "#     .csv(\"/home/jovyan/data/gamescore\")\n",
    "    \n",
    "    \n",
    "# # create the event time column \n",
    "# withEventTimedf = sdf.selectExpr(\n",
    "#     \"*\",\n",
    "#     \"cast(timestamp_in_ms/1000.0 as timestamp) as event_time\")\n",
    "\n",
    "# withEventTimedf.printSchema()\n",
    "\n",
    "# avgscoredf = withEventTimedf \\\n",
    "#     .groupBy(window(col(\"event_time\"), \"10 seconds\"), \"uname\", \"tname\") \\\n",
    "#     .agg(avg(\"score\").alias(\"value\"))\n",
    "\n",
    "# resultdf = avgscoredf.select(concat(col(\"uname\"), lit(\" \"), col(\"tname\")).alias(\"key\"), col(\"value\"))\n",
    "\n",
    "# query = resultdf \\\n",
    "#     .writeStream \\\n",
    "#     .queryName(\"avg_score_window\") \\\n",
    "#     .format(\"memory\") \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .start()\n",
    "\n",
    "# try:\n",
    "#     for x in range(100):\n",
    "#         spark.sql(\"SELECT * FROM avg_score_window\").show()\n",
    "#         sleep(10)\n",
    "# except KeyboardInterrupt:\n",
    "#     query.stop()\n",
    "#     # Stop the spark context\n",
    "#     spark.stop()\n",
    "#     print(\"Stoped the streaming query and the spark context\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ca3967508393c830e5fdf827e244c60a1e86d5fcaf2e086715c4929113c1ab3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
